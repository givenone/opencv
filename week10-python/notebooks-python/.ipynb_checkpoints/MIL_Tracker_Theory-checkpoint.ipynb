{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Object Tracking using Multiple Instance Learning</font>\n",
    "\n",
    "# <font style = \"color:rgb(50,120,229)\">Overview</font>\n",
    "\n",
    "We take this opportunity to discuss a weakly supervised learning framework called [Multiple Instance Learning](https://en.wikipedia.org/wiki/Multiple_instance_learning) ( MIL ). It deals with uncertainty in training labels. We will discuss how MIL is used for object tracking. \n",
    "\n",
    "# <font style = \"color:rgb(50,120,229)\">Object Tracking using MIL</font>\n",
    "\n",
    "We will discuss the framework proposed by Boris Babenko *et al.* in their [paper](http://vision.ucsd.edu/~bbabenko/data/miltrack_cvpr09.pdf) for Object tracking using Online Multiple Instance Learning.\n",
    "\n",
    "Traditional supervised learning based methods train a discriminative classifier to separate the object from the background. The first frame in the sequence is provided with the bounding box of the object to be tracked. Then, positive and negative examples are extracted from this object window. Errors in the position of the selected window can lead to incorrectly labeled training examples. If the training examples are not good in a supervised learning setup, the classifier learnt will not perform as expected. Moreover, well annotated data can be costly to get. \n",
    "\n",
    "Keeping this in mind, a weakly supervised learning approach called Multiple Instance Learning is used where we don’t have full confidence in the training data. The proposed system consists of three components : \n",
    "\n",
    "1. Image representation \n",
    "\n",
    "2. Motion Model\n",
    "\n",
    "3. Appearance Model\n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Image representation</font>\n",
    "\n",
    "For each image patch, Haar-like features are computed which have been discussed in great detail in the previous module. These features are also used as weak classifiers for the online learning algorithm as discussed later.\n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Motion model</font>\n",
    "\n",
    "A very naive motion model is used where the location of the tracker window at time $t$ is equally likely to appear within a radius s of the tracker location at time $t-1$. Basically, it says that the new window in the current frame should be in the neighborhood of the the window of previous frame. \n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Appearance Model</font>\n",
    "\n",
    "A discriminative classifier based on Multiple Instance Learning is used as the appearance model. \n",
    "\n",
    "### <font style = \"color:rgb(50,120,229)\">Multiple Instance Learning -- In words</font>\n",
    "\n",
    "In MIL, we do not specify positive and negative examples, but positive and negative sets called \"bags\". The collection of images in the positive bag are not strictly positive examples. Instead, a bag is labeled positive if at least one image in the bag is a positive example and negative if all examples in the bag are negative. In case of tracking, a positive bag contains the patch centered on the current location of the object and also patches in a small neighborhood around it. Even if the current location of the tracked object is not accurate, when samples from the neighborhood of the current location are put in the positive bag, there is a good chance that this bag contains at least one image in which the object is nicely centered.\n",
    "\n",
    "### <font style = \"color:rgb(50,120,229)\">Training data creation</font>\n",
    "\n",
    "Let us define some variables for better understanding. Let $x$ be an image patch, $l(x)$ be its location and ${l_t}^*$ be the tracked location of the object at frame $t$. A set of image patches, given by $X^s$ are extracted from within a search window of radius $s$ around the current tracker location $({l^*}_{t-1})$. The classifier should be able to find the probability $p(y=1|x)$ which indicates the probability of an image patch being object $(y=1)$ or background $(y=0)$. This will be discussed in the next section. The updated tracker location is found by maximizing the above probability and can be written as :\n",
    "\n",
    "$$\n",
    "l^*_t = l( \\underset{x \\in X^s}{\\arg\\max}\\, p(y|x))\n",
    "$$\n",
    "\n",
    "Basically, we find the probability of each patch being the object of interest and take the patch whose probability comes out to be highest. \n",
    "\n",
    "Once the updated location is found, we can create the image patches for the positive and negative bags to be supplied to the classifier. Image patches are extracted from a window around ${l_t}^*$ and put in the positive bag. The region complementary to the positive examples is used to crop image patches for negative bag. The positive and negative bag is then used for updating the classifier.\n",
    "\n",
    "\n",
    "| <center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w8-m5-posImageExample.png\" width=500/></center> | \n",
    "| -------- | \n",
    "| <center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w8-m5-posNegBag.png\" width=500/></center>     | \n",
    "|Figure showing the generation of positive and negative bags for a given frame. The solid green rectangle is the position of the tracked window in the current frame. The dashed rectangle form the neighborhood of the object of interest. These patches are cropped and put in the bag of positive instances The Red rectangles are the patches taken for negative instances.|\n",
    "\n",
    "### <font style = \"color:rgb(50,120,229)\">Multiple Instance Learning -- In equations</font>\n",
    "\n",
    "In traditional supervised learning framework for binary classification, the data is provided as tuples of training instance and its label. The dataset can be described as $\\{(x_1, y_1), (x_2, y_2),... (x_n, y_n)\\}$. Where, $x_i$ is an instance of positive or negative class and $y_i$ is the label which can be either 0 or 1. \n",
    "\n",
    "In the MIL framework, as mentioned earlier, there is a notion of sets of instances called \"bags\". Each bag contains a number of instances. The training data is represented as $\\{(X_1, y_1), (X_2, y_2),... (X_n, y_n)\\}$ where, $X_i$ is the bag which contains instances $\\{x_1, x_2,... x_n\\}$ and $y_i$ are the bag label. $y_i$ is obtained as $y_i = max(y_{ij})$, where $y_{ij}$ are the instance labels. So, if any of the $y_{ij}$ is found to be 1, then the corresponding $y_i$ is labeled 1, otherwise 0.\n",
    "\n",
    "The classifier needs to predict $p(y|x)$. For that we need to minimize the loss function given by :\n",
    "$$\n",
    "\\log{L} = \\sum_i(\\log{p(y_i|X_i)}) \\hspace{5cm}(1)\n",
    "$$\n",
    "\n",
    "And $p(y_i|X_i)$ is in terms of bags. We need to represent the above loss function in terms of instances while maintaining the property that the bag will have high probability if any of the instance has high probability. This is done by using the noisy-OR model which is given by\n",
    "\n",
    "$$\n",
    "p(y_i|X_i) = 1 − \\prod_j(1 − p(y_i|x_{ij}))\\hspace{3.5cm}(2)\n",
    "$$\n",
    "\n",
    "### <font style = \"color:rgb(50,120,229)\">Online Boosting</font>\n",
    "\n",
    "Boosting was explained in the previous module as a method of combining several weak classifiers $(h_i(x))$ into a strong classifier.\n",
    "\n",
    "$$\n",
    "\\bf{h(x)} = \\bf{sign}(\\sum_i (\\alpha_i h_i(x))\n",
    "$$\n",
    "\n",
    "where, $h(x)$ is the strong classifier and $\\alpha_i$ are the weights which indicate the contribution of each weak classifier towards the final classifier. They are learnt during the training process. After each weak classifier is trained, the weights are adjusted such that the samples which were previously misclassified receive more weight.\n",
    "\n",
    "The training of these classifiers is done in an online manner, i.e. the classifier keeps on updating itself as the new frames arrive. The weak classifiers are chosen sequentially by optimizing equation (2).\n",
    "\n",
    "**<font style=\"color:rgb(255,0,0)\">NOTE :</font>** The code and tutorial for this tracker is given in the previous chapter.\n",
    "\n",
    "### <font style = \"color:rgb(50,120,229)\">References and Further Reading</font>\n",
    "\n",
    "1. [http://vision.ucsd.edu/~bbabenko/data/miltrack_cvpr09.pdf](http://vision.ucsd.edu/~bbabenko/data/miltrack_cvpr09.pdf)\n",
    "\n",
    "2. [http://vision.stanford.edu/teaching/cs231b_spring1415/slides/MIL_kelsie.pdf](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/MIL_kelsie.pdf)\n",
    "\n",
    "3. [http://docs.opencv.org/trunk/d0/d26/classcv_1_1TrackerMIL.html](http://docs.opencv.org/trunk/d0/d26/classcv_1_1TrackerMIL.html)\n",
    "\n",
    "4. [https://en.wikipedia.org/wiki/Multiple_instance_learning](https://en.wikipedia.org/wiki/Multiple_instance_learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
